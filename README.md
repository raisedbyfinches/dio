# Dio

A rather tenuous name. Branding functions with a "created by AI" message and attributes to help identify them.

Brand -> Brando -> Dio Brando -> Dio -> [Za Warudo](https://jojo.fandom.com/wiki/The_World)

This is (currently: will be) a repository of useful functions and tools for working with code generated by AI tools. 


## Installation

For python installation I recommend managing `dio` as a dependency using [poetry](https://python-poetry.org/) or [uv](https://docs.astral.sh/uv/). You can add it to your project dependencies using one of the following.

#### Poetry

Add it to your pyproject.toml with:

``` toml
[tool.poetry.dependencies]
dio = { git = "https://github.com/raisedbyfinches/dio.git", branch = "python" }
```

#### uv

Install it using the CLI:

```sh
uv pip install "git+https://github.com/raisedbyfinches/dio@python"
```


## Usage

### AI generated code

When developing code with Copilot assistance we should mark all code written by it. To do so follow this example and apply it to your own codebase.

``` python
from dio import ai


@ai(verbose=True)
def add(x, y):
    return x + y


@ai(llm="claude")
def example(x: int, y: int) -> int:
    "Demo."
    result = 0
    for _ in range(x):
        if y > 0:
            if y < 10:
                result += y
            else:
                result *= y
        else:
            result -= y

    return result


@ai(verbose=True, log_file="./log.txt")
def example_(x: int, y: int) -> int:
    "Demo."
    result = 0
    for _ in range(x):
        if y:
            result += y

    return result


if __name__ == "__main__":
    simple = add(3, 5)
    result = example(3, 5)
    result_ = example_(3, 5)

    print(f"\nResult: {result}")
    print("\nFunction metadata:")
    print(f"    - Complexity: {example.complexity}")
    print(f"    - Anti-patterns: {example.anti_patterns}")
    print(f"    - System info: {example.system}")

    print(f"\nResult: {result_}")
    print("\nFunction metadata:")
    print(f"    - Complexity: {example_.complexity}")
    print(f"    - Anti-patterns: {example_.anti_patterns}")
    print(f"    - System info: {example_.system}")

```

This is included as a standalone file in `example.py` if you would like to tinker with it.

This is explicitly copilot as the default value as that's the tool available in my work setting. Other LLM origins are not supported *yet*.

The `@ai` decorator provides:

This provides
- A warning that a function call is using a function which was written with or by Microsoft Copilot, including the function call itself.
- Attributes to the function itself to better frame your questions when investigating the content.
  + Code complexity (via cyclomatic complexity computation)
  + Identification of long line and deeply nested (list + conditional) anti-patterns
  + Information for the system on which the function is called

However, this doesn't come for free. There are performance implications of calculation the metrics. There is LRU caching mechanism to ensure that the calculations only occur once. Any hit to performance depends on the code it is assessing. In particular, AST traversal can be costly in complex cases. If your function is particularly deep or "branchy" then this can take more time than for less complicated functions. This is cached so that it only gets calculated at the definition site. Having an LLM write a large number of your functions or particularly complex functions will thus impose a greater penalty on performance. It is still vital that these functions are demarkated as not written by humans.
